# üé≠ Production-Ready Web Scraping System

A modern, scalable web scraping system built with **Playwright**, **BullMQ**, **Hono**, and **Bun**. This system demonstrates production-ready architecture for automated web scraping with queue management, RESTful API, and process monitoring.

## üèóÔ∏è Architecture Overview

```mermaid
graph TD
    A[Client] --> B[Hono API Server]
    B --> C[BullMQ Queue]
    C --> D[Redis]
    E[Worker Process] --> C
    E --> F[Playwright Scrapers]
    F --> G[Database]
    F --> H[Markdown Files]
    I[PM2 Process Manager] --> B
    I --> E
```

## üöÄ What We Built

### **Core Components:**

1. **üé≠ Playwright Scrapers**
   - Automated browser control for dynamic content
   - Support for quotes, Reddit, TikTok (extensible)
   - Converts scraped data to clean Markdown format

2. **üêÇ BullMQ Queue System**
   - Reliable job queue with Redis backend
   - Automatic retries for failed jobs
   - Job progress tracking and monitoring

3. **üî• Hono REST API**
   - Lightning-fast API server built on Bun
   - Endpoints for triggering scrapes, monitoring jobs, retrieving data
   - Health checks and system statistics

4. **üíæ SQLite Database**
   - Persistent storage for scraped content
   - Job tracking and status management
   - Full-text search capabilities

5. **üöÄ PM2 Process Management**
   - Automatic process restarts on crashes
   - Log management and monitoring
   - Production-ready deployment

## üìÅ Project Structure

```
Scraper Script/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/         # üé≠ Playwright scrapers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ quotes-scraper.ts
‚îÇ   ‚îú‚îÄ‚îÄ workers/          # üêÇ BullMQ job processors  
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraper-worker.ts
‚îÇ   ‚îú‚îÄ‚îÄ api/              # üî• Hono REST API
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server.ts
‚îÇ   ‚îú‚îÄ‚îÄ database/         # üíæ SQLite database setup
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.ts
‚îÇ   ‚îî‚îÄ‚îÄ config/           # ‚öôÔ∏è Configuration files
‚îÇ       ‚îú‚îÄ‚îÄ redis.ts
‚îÇ       ‚îî‚îÄ‚îÄ bullmq.ts
‚îú‚îÄ‚îÄ data/                 # üìä Scraped results & database
‚îú‚îÄ‚îÄ logs/                 # üìù Application logs  
‚îú‚îÄ‚îÄ server.ts             # üî• API server entry point
‚îú‚îÄ‚îÄ worker.ts             # üêÇ Worker entry point
‚îú‚îÄ‚îÄ ecosystem.config.cjs  # üöÄ PM2 configuration
‚îî‚îÄ‚îÄ package.json          # üì¶ Dependencies & scripts
```

## üîß Quick Start

### **Prerequisites**
- Bun installed (`curl -fsSL https://bun.sh/install | bash`)
- Redis running (`brew install redis && brew services start redis`)
- PM2 for production (`bun add -g pm2`)

### **Installation & Setup**
```bash
# Clone and setup
cd "Scraper Script"
bun install

# Create directories
mkdir -p data logs

# Test the basic scraper
bun run src/scrapers/quotes-scraper.ts
```

### **Development Mode**
```bash
# Terminal 1: Start API Server
bun run dev:server

# Terminal 2: Start Worker
bun run dev:worker

# Test the API
curl http://localhost:3000/health
```

### **Production Deployment**
```bash
# Start with PM2
bun run pm2:start

# Monitor processes
bun run pm2:status
bun run pm2:logs

# Stop all processes
bun run pm2:stop
```

## üéØ API Endpoints

### **Core Endpoints**
| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Welcome & API documentation |
| `GET` | `/health` | System health check |
| `POST` | `/scrape` | Trigger scraping job |
| `GET` | `/jobs/:id` | Get job status |
| `GET` | `/jobs` | List all jobs |
| `GET` | `/results` | Get scraped content |
| `GET` | `/stats` | System statistics |

### **Quick Test Endpoints**
| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/quick/quotes` | Start quotes scraping |

### **Example API Usage**

**Trigger a scraping job:**
```bash
curl -X POST http://localhost:3000/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "type": "scrape-quotes",
    "url": "http://quotes.toscrape.com",
    "pages": 3
  }'
```

**Check job status:**
```bash
curl http://localhost:3000/jobs/YOUR_JOB_ID
```

**Get scraped results:**
```bash
curl "http://localhost:3000/results?source=quotes&limit=10"
```

## üé≠ Available Scrapers

### **1. Quotes Scraper** ‚úÖ (Complete)
- **Target:** `http://quotes.toscrape.com`
- **Data:** Quote text, authors, tags
- **Format:** Beautiful Markdown output
- **Features:** Multi-page scraping, rate limiting

### **2. Reddit Scraper** üöß (Planned)
- **Target:** `https://reddit.com`
- **Data:** Posts, comments, metadata
- **Features:** Subreddit filtering, comment threading

### **3. TikTok Scraper** üöß (Bonus Feature)
- **Target:** `https://tiktok.com`
- **Data:** Video metadata, descriptions, hashtags
- **Features:** Anti-bot evasion, rate limiting

## üìä Data Output Examples

### **Markdown Format**
```markdown
# üìö Scraped Quotes Collection

*Scraped 20 quotes on 2025-08-24*

---

## Quote 1

> "The world as we have created it is a process of our thinking..."

**Author:** Albert Einstein
**Tags:** `change`, `deep-thoughts`, `thinking`, `world`
```

### **JSON API Response**
```json
{
  "success": true,
  "results": [
    {
      "id": 1,
      "source": "quotes",
      "url": "http://quotes.toscrape.com",
      "title": null,
      "content": "> \"The world as we have created it...\"",
      "scrapedAt": "2025-08-24T12:00:00.000Z",
      "metadata": {
        "author": "Albert Einstein",
        "tags": ["change", "deep-thoughts"]
      }
    }
  ]
}
```

## ‚öôÔ∏è Configuration

### **Environment Variables**
```bash
# Server Configuration
PORT=3000
NODE_ENV=production

# Redis Configuration  
REDIS_HOST=127.0.0.1
REDIS_PORT=6379

# Worker Configuration
WORKER_CONCURRENCY=2
```

### **BullMQ Settings**
- **Default Retries:** 3 attempts
- **Retry Delay:** Exponential backoff (2s, 4s, 8s)
- **Job Retention:** 50 completed, 20 failed jobs
- **Concurrency:** 2 simultaneous jobs

## üîç Monitoring & Logs

### **PM2 Monitoring**
```bash
# Real-time monitoring dashboard
pm2 monit

# Process status
pm2 status

# View logs
pm2 logs

# Restart all processes
pm2 restart all
```

### **Log Files**
- `logs/api-out.log` - API server output
- `logs/api-error.log` - API server errors
- `logs/worker-out.log` - Worker output
- `logs/worker-error.log` - Worker errors

### **Health Monitoring**
```bash
# Check system health
curl http://localhost:3000/health

# Get system statistics
curl http://localhost:3000/stats
```

## üßπ Maintenance Commands

```bash
# Clean up data and logs
bun run clean

# Restart PM2 processes
bun run pm2:restart

# Update dependencies
bun update

# View queue statistics
curl http://localhost:3000/stats
```

---

*Built with ‚ù§Ô∏è using Bun, Playwright, Hono, BullMQ, and modern JavaScript*